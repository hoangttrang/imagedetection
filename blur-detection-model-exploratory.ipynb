{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training NB: \n",
    "This notebook is used to create a data loader for the images along with creating a CNN model to train specifically to detect clear and blurry images from the dataset. \n",
    "Here are the steps that I did in this notebook\n",
    "- Create Data Loader for train, valid and test. There are more blurry images than clear one so I sampled extra clear images (with different image transformation through reversing the image). This way, the dataset for train and valid would be balanced \n",
    "- Create a base model: with a simple CNN architecture, I want to see how well the model performs \n",
    "- Train model with CV2 model or CLIP-IQA\n",
    "    - Model Evaluation should include: accuracy score, and confusion matrix \n",
    "\n",
    "**Notes**: So far, for this notebook, I have finished the data loader, and create a base model. There are still a lot of work left for CV2 and CLIP-IQA as I need need to read their codes and documentations to figure out how can I change the code such that it can be applied for this problem\n",
    "\n",
    "### Import data and create a data loader \n",
    "The image for this is resize to 224x224 as it is a common dimension for most models. The resize is to scale the image down. Another resize approach that I want to test is keeping the aspect ratio of the image while scaling down and then pad the images so it would fit in the 224x224 dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch \n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data files and folders\n",
    "current_wd = os.getcwd()\n",
    "data_file = f'{current_wd}/data'\n",
    "\n",
    "blurry_folder = os.path.join(data_file, 'blurry')\n",
    "clear_folder = os.path.join(data_file, 'clear')\n",
    "test_folder = os.path.join(data_file, 'to-be-classified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  label\n",
       "0  c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...      1\n",
       "1  c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...      1\n",
       "2  c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...      1\n",
       "3  c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...      1\n",
       "4  c:\\Users\\tata\\Documents\\Vanderbilt_research_pr...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create metadata for clear and blurry images\n",
    "def create_metadata(clear_folder, blurry_folder):\n",
    "    metadata = []\n",
    "    for img_name in os.listdir(clear_folder):\n",
    "        metadata.append({'image_path': os.path.join(clear_folder, img_name), 'label': 1})\n",
    "    for img_name in os.listdir(blurry_folder):\n",
    "        metadata.append({'image_path': os.path.join(blurry_folder, img_name), 'label': 0})\n",
    "    return pd.DataFrame(metadata)\n",
    "\n",
    "images_path_w_metadata = create_metadata(clear_folder, blurry_folder)\n",
    "images_path_w_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split clear and blurry images into train and validation sets\n",
    "def split_data(metadata, train_size=200, valid_size=40):\n",
    "    clear_images = metadata[metadata['label'] == 1]\n",
    "    blurry_images = metadata[metadata['label'] == 0]\n",
    "\n",
    "    clear_train, clear_valid = train_test_split(clear_images, random_state=42)\n",
    "    blurry_train, blurry_valid = train_test_split(blurry_images, random_state=42)\n",
    "\n",
    "    train_metadata = pd.concat([clear_train, blurry_train]).sample(frac=1).reset_index(drop=True)\n",
    "    valid_metadata = pd.concat([clear_valid, blurry_valid]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return train_metadata, valid_metadata\n",
    "\n",
    "train_metadata, valid_metadata = split_data(images_path_w_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear train:  64\n",
      "blurry train:  123\n",
      "clear valid:  22\n",
      "blurry valid:  42\n"
     ]
    }
   ],
   "source": [
    "clear_train = train_metadata[train_metadata['label'] == 1]\n",
    "blurry_train = train_metadata[train_metadata['label'] == 0]\n",
    "clear_valid = valid_metadata[valid_metadata['label'] == 1]\n",
    "blurry_valid = valid_metadata[valid_metadata['label'] == 0]\n",
    "print(\"clear train: \", len(clear_train))\n",
    "print(\"blurry train: \", len(blurry_train))\n",
    "print(\"clear valid: \", len(clear_valid))\n",
    "print(\"blurry valid: \", len(blurry_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Balance datasets by augmenting images\n",
    "def augment_data(metadata, target_size):\n",
    "    current_size = len(metadata)\n",
    "    if current_size >= target_size:\n",
    "        return metadata\n",
    "\n",
    "    augment_needed = target_size - current_size\n",
    "    augmented_metadata = metadata.sample(augment_needed, replace=True, random_state=42)\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10)\n",
    "    ])\n",
    "    augmented_images = []\n",
    "    for idx, row in augmented_metadata.iterrows():\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        image = augment_transform(image)\n",
    "        # Save augmented images temporarily in a list with their label\n",
    "        augmented_images.append({'image': image, 'label': row['label']})\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_images)\n",
    "    return pd.concat([metadata, augmented_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_metadata = pd.concat([\n",
    "    augment_data(clear_train, len(blurry_train)),\n",
    "    augment_data(blurry_train, len(blurry_train))\n",
    "]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "valid_metadata = pd.concat([\n",
    "    augment_data(clear_valid, len(blurry_valid)),\n",
    "    augment_data(blurry_valid, len(blurry_valid))\n",
    "]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Dataset class with OpenCV conversion\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, metadata, transform=None):\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if 'image' in self.metadata.iloc[idx] and isinstance(self.metadata.iloc[idx]['image'], Image.Image):\n",
    "            image = self.metadata.iloc[idx]['image']\n",
    "        else:\n",
    "            img_path = self.metadata.iloc[idx]['image_path']\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = self.metadata.iloc[idx]['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloader(metadata, transform, batch_size=32, shuffle=True):\n",
    "    dataset = ImageDataset(metadata, transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_loader = create_dataloader(train_metadata, transform)\n",
    "valid_loader = create_dataloader(valid_metadata, transform)\n",
    "\n",
    "test_metadata = pd.DataFrame([{'image_path': os.path.join(test_folder, img_name)} for img_name in os.listdir(test_folder) if os.path.isfile(os.path.join(test_folder, img_name))])\n",
    "test_loader = create_dataloader(test_metadata, transform, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts in the training set: Counter({1: 123, 0: 123})\n",
      "Label counts in the validation set: Counter({0: 42, 1: 42})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count labels from a DataLoader\n",
    "def count_labels_from_dataloader(dataloader):\n",
    "    label_counter = Counter()\n",
    "    for _, labels in dataloader:\n",
    "        label_counter.update(labels.numpy())\n",
    "    return label_counter\n",
    "\n",
    "train_label_counts = count_labels_from_dataloader(train_loader)\n",
    "valid_label_counts = count_labels_from_dataloader(valid_loader)\n",
    "\n",
    "print(\"Label counts in the training set:\", train_label_counts)\n",
    "print(\"Label counts in the validation set:\", valid_label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "### Simple CNN: \n",
    "\n",
    "The `SimpleBlurDetectionModel` class is designed for detecting blur in images using a neural network. It provides the following functionalities:\n",
    "\n",
    "- **Initialization**:\n",
    "  - Configures the model with specified neurons, data loaders, learning rate, maximum iterations, and early stopping patience.\n",
    "  - Utilizes GPU if available.\n",
    "\n",
    "- **Training**:\n",
    "  - Trains the neural network using the provided training data loader.\n",
    "  - Implements early stopping based on validation loss to prevent overfitting.\n",
    "  - Logs training and validation loss for each epoch.\n",
    "\n",
    "- **Validation**:\n",
    "  - Evaluates the model's performance on the validation set.\n",
    "  - Computes and returns the average validation loss.\n",
    "\n",
    "- **Evaluation**:\n",
    "  - Loads the best model parameters saved during training.\n",
    "  - Calculates and prints the accuracy of the model on the validation set.\n",
    "  - Generates and displays a confusion matrix for detailed performance analysis.\n",
    "\n",
    "- **Model Management**:\n",
    "  - Provides methods to save and load model weights for future use.\n",
    "\n",
    "This class ensures a streamlined workflow for training, validating, and evaluating a blur detection neural network, making it a valuable tool for image quality assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBlurDetectionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleBlurDetectionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path=f'{os.getcwd()}/checkpoint.pth'):\n",
    "        \"\"\"\n",
    "        Early stops the training if validation loss doesn't improve after a given patience.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        self.best_loss = val_loss\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBlurDetectionModel:\n",
    "    def __init__(self, neurons, train_loader, valid_loader, learning_rate=0.001, max_iter=50, patience=10):\n",
    "        self.neurons = neurons\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.patience = patience\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = SimpleBlurDetectionNN(neurons[0], neurons[1], neurons[2]).to(self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.early_stopping = EarlyStopping(patience=self.patience, verbose=True, path=f'{os.getcwd()}/best_model.pth')\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.max_iter):\n",
    "            epoch_loss = 0\n",
    "            self.model.train()  # Set the model to training mode\n",
    "            for images, labels in self.train_loader:\n",
    "                images = images.view(images.size(0), -1).to(self.device)\n",
    "                labels = labels.view(-1, 1).float().to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            valid_loss = self.validate()\n",
    "            print(f'Epoch [{epoch+1}/{self.max_iter}], Train Loss: {epoch_loss/len(self.train_loader):.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "            self.early_stopping(valid_loss, self.model)\n",
    "\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.valid_loader:\n",
    "                images = images.view(images.size(0), -1).to(self.device)\n",
    "                labels = labels.view(-1, 1).float().to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        return valid_loss / len(self.valid_loader)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.load_state_dict(torch.load('best_model.pth'))  # Load the best model\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.valid_loader:\n",
    "                images = images.view(images.size(0), -1).to(self.device)\n",
    "                labels = labels.view(-1, 1).float().to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                print(\"predicted: \", predicted)\n",
    "                print(\"labels: \", labels)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy of the model on the validation images: {accuracy:.2f}%')\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.7262, Validation Loss: 0.7660\n",
      "Validation loss decreased (0.765952 --> 0.765952).  Saving model ...\n",
      "Epoch [2/50], Train Loss: 0.7043, Validation Loss: 0.7534\n",
      "Validation loss decreased (0.753410 --> 0.753410).  Saving model ...\n",
      "Epoch [3/50], Train Loss: 0.7014, Validation Loss: 0.7668\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [4/50], Train Loss: 0.6990, Validation Loss: 0.7675\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch [5/50], Train Loss: 0.7036, Validation Loss: 0.7479\n",
      "Validation loss decreased (0.747939 --> 0.747939).  Saving model ...\n",
      "Epoch [6/50], Train Loss: 0.6965, Validation Loss: 0.7227\n",
      "Validation loss decreased (0.722748 --> 0.722748).  Saving model ...\n",
      "Epoch [7/50], Train Loss: 0.6946, Validation Loss: 0.7021\n",
      "Validation loss decreased (0.702143 --> 0.702143).  Saving model ...\n",
      "Epoch [8/50], Train Loss: 0.6918, Validation Loss: 0.7351\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [9/50], Train Loss: 0.6860, Validation Loss: 0.7440\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch [10/50], Train Loss: 0.6871, Validation Loss: 0.6938\n",
      "Validation loss decreased (0.693824 --> 0.693824).  Saving model ...\n",
      "Epoch [11/50], Train Loss: 0.6806, Validation Loss: 0.6866\n",
      "Validation loss decreased (0.686630 --> 0.686630).  Saving model ...\n",
      "Epoch [12/50], Train Loss: 0.6710, Validation Loss: 0.7021\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [13/50], Train Loss: 0.6822, Validation Loss: 0.6945\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch [14/50], Train Loss: 0.6799, Validation Loss: 0.7000\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch [15/50], Train Loss: 0.6792, Validation Loss: 0.6998\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch [16/50], Train Loss: 0.6799, Validation Loss: 0.6981\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "predicted:  tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "labels:  tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "predicted:  tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "labels:  tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "predicted:  tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "labels:  tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "Accuracy of the model on the validation images: 55.95%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming train_loader and valid_loader are already defined and available\n",
    "neurons = (3 * 224 * 224, 128, 1)  # Assuming input images are 224x224 with 3 channels\n",
    "model = SimpleBlurDetectionModel(neurons, train_loader, valid_loader, max_iter=50, patience=5)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate()\n",
    "\n",
    "# Save the final model\n",
    "model.save_model('blur_detection_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments: \n",
    "The model shows some capability in classifying images, its performance is limited by a significant number of false negatives and a relatively low accuracy. I want to compare this with other models that specifically used to train for bluriness detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blurry_clear_imagedetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
